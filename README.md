# TinyML Customer Feedback Detection on UNIHIKER K10 (ESP32‑S3)

A project that captures live audio on the UNIHIKER K10, classifies customer feedback categories from speech with a quantized CNN generated by Edge Impulse, and visualizes results on both the device TFT and a Tailwind‑styled web dashboard served directly from the board.

This repository includes:
- Complete firmware source code (PlatformIO) under `src/` and the Edge Impulse library under `lib/Speech_Emotion_inferencing/`.
- A detailed development log: dataset preparation, model training, and application development.

## Hardware and software

- Board: UNIHIKER K10 (ESP32‑S3 + ES7210 audio codec + 240×320 TFT)
- Microphone: On‑board digital mic via I2S (TDM, down‑mixed to mono, 16 kHz)
- Tooling: Edge Impulse Studio (dataset + training + C++ SDK export), PlatformIO (build/flash)

## Quick start

1) Clone and open this repo in VS Code.
2) Configure Wi‑Fi (for web dashboard): set `WIFI_SSID` and `WIFI_PASSWORD` at the top of `src/main.cpp`.

3) Build and upload with PlatformIO.

4) After boot, the TFT shows the assigned IP (also printed to serial). Open that address in a browser to view the Tailwind dashboard.

## Dataset preparation

This project follows the flow recommended by TinyML literature for voice‑based classification, notably using Mel‑Filterbank Energy (MFE) features and a compact CNN classifier. We adapt the same technique used in speech‑emotion research to customer‑feedback detection. See the reference paper at the end of this README.

- Classes: map your target feedback categories to a small, distinct set (e.g., {positive, neutral, negative, complaint}). The model in this repo expects `EI_CLASSIFIER_LABEL_COUNT` labels; update labels/model together if you change them.
- Collection: record short utterances (1–3 s) in varied conditions (speakers, accents, environments). Aim for class balance.
- Labeling: label each sample by the intended feedback category (e.g., positive/neutral/negative/complaint) rather than raw emotion.
- Split: train/validation/test with speaker‑independent splits (e.g., 70/15/15) to avoid speaker leakage.
- Augmentation: mild background noise, gain jitter, and time shift are effective. Avoid over‑augmentation that changes prosody.
- Sampling: 16 kHz mono is a good trade‑off for prosodic cues and MCU throughput.

Edge Impulse specifics:
- In the Studio, create an Impulse with MFE (or MFCC if preferred) as the DSP block. Parameters used in our training: window length 25 ms, window stride 10 ms, 20–40 mel bands (project dependent), Hamming window.
- Normalize per‑feature (per‑axis) and enable quantization‑aware training (QAT) for int8 export.

## Model training (Edge Impulse)

- Architecture: compact 1D/2D CNN over time–frequency features with batch norm and dropout. Keep parameters small (< ~250k) to fit memory/speed on ESP32‑S3.
- Training: Adam optimizer, initial LR 1e‑3 with cosine/step decay; 30–100 epochs; early stopping on validation loss.
- Quantization: int8 post‑training with QAT enabled to preserve accuracy.
- Metrics: report overall accuracy, per‑class F1, and a confusion matrix on the held‑out test set.
- On‑device compute: ensure DSP + inference latency comfortably fits your application cadence (our loop runs ~2 Hz with full UI updates; adjust delays as needed).

Export the library via “Deploy → Arduino library” (Edge Impulse), then place it under `lib/Speech_Emotion_inferencing/` (already present here).

## Application development

Key elements live in `src/main.cpp`:

- Audio capture: uses UNIHIKER K10’s codec via I2S (legacy API). Stereo frames are down‑mixed to mono (left channel) at 16 kHz.
- Feature pipeline: sample buffer passed into Edge Impulse `run_classifier` with MFE features compiled into the library.
- TFT UI: a minimal on‑device dashboard draws a label/score bar chart and shows the board IP along the top.
- Wi‑Fi + Web dashboard:
	- Connects to your AP using `WIFI_SSID`/`WIFI_PASSWORD`.
	- Starts an HTTP server and serves a Tailwind‑styled dashboard at `/`.
	- Provides a JSON endpoint at `/data` exposing: `labels`, `probabilities`, `counts`, `topLabel`, `inferenceCount`, `humanTimestamp`, and `ip`.
	- The web UI refreshes every 1.5 s and animates bars for an “at‑a‑glance” view.

Endpoints:
- `/` – Tailwind HTML dashboard (served from flash/PROGMEM)
- `/data` – JSON snapshot for custom dashboards or external clients

## Building and flashing

Environment is defined in `platformio.ini`:

```ini
[env:unihiker]
platform = https://github.com/DFRobot/platform-unihiker.git
board = unihiker_k10
framework = arduino
lib_extra_dirs = lib
build_flags =
		-DARDUINO_USB_CDC_ON_BOOT=1
		-DARDUINO_USB_MODE=1
		-DModel=None
		-DEIDSP_USE_ESP_DSP=0
```

Typical workflow:

```bash
# build
pio run

# upload (adjust the env if needed)
pio run -t upload

# open serial monitor at 115200 baud
pio device monitor -b 115200
```